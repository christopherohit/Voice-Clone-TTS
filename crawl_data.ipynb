{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from other python files and scrapy files and the needed csv file containing all URLs/proxies/ua\n",
    "import csv\n",
    "import scrapy\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy_splash import SplashRequest\n",
    "#from scrapy.http import Request\n",
    "##########################          SPALSHSPIDER.PY OVERVIEW      #####################################################\n",
    "# process the csv file so the url + ip address + useragent pairs are the same as defined in the file\n",
    "# returns a list of dictionaries, example:\n",
    "# [ {'url': 'http://www.starcitygames.com/catalog/category/Rivals%20of%20Ixalan',\n",
    "#    'ip': 'http://204.152.114.244:8050',\n",
    "#    'ua': \"Mozilla/5.0 (BlackBerry; U; BlackBerry 9320; en-GB) AppleWebKit/534.11\"},\n",
    "#    ...\n",
    "# ]\n",
    "# plus python file also scrapes all URLs returning needed info and goes to all apges associated with URL by clicking next button\n",
    "\n",
    "# Function to read csv file that contains URLs that are paried with proxies and user agents\n",
    "def process_csv(csv_file):\n",
    "    # Initialize data\n",
    "    data = []\n",
    "    # Initialize reader\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader)\n",
    "\n",
    "    # While inside csv file and not at end of csv file\n",
    "    for fields in reader:\n",
    "\n",
    "        # Set URL\n",
    "        if fields[0] != \"\":\n",
    "            url = fields[0]\n",
    "        else:\n",
    "            continue # skip the whole row if the url column is empty\n",
    "        #Set proxy and pair with correct URL\n",
    "        if fields[1] != \"\":\n",
    "            ip = \"http://\" + fields[1] + \":8050\" # adding http and port because this is the needed scheme\n",
    "        # Set user agent and pair with correct URL\n",
    "        if fields[2] != \"\":\n",
    "            useragent = fields[2]\n",
    "        # Put all three together\n",
    "        data.append({\"url\": url, \"ip\": ip, \"ua\": useragent})\n",
    "    # Return URL paried with ua and proxy\n",
    "    return data\n",
    "\n",
    "# Spider class\n",
    "class MySpider(Spider):\n",
    "# Name of Spider\n",
    "    name = 'splash_spider'\n",
    "    # getting all the url + ip address + useragent pairs then request them\n",
    "    def start_requests(self):\n",
    "        # get the file path of the csv file that contains the pairs from the settings.py\n",
    "        with open(self.settings[\"PROXY_CSV_FILE\"], mode=\"r\") as csv_file:\n",
    "           # requests is a list of dictionaries like this -> {url: str, ua: str, ip: str}\n",
    "            requests = process_csv(csv_file)\n",
    "            for i, req in enumerate(requests):\n",
    "                x = len(requests) - i  # <- check here\n",
    "                # Return needed url with set delay of 3 seconds\n",
    "                yield SplashRequest(url=req[\"url\"], callback=self.parse, args={\"wait\": 3},\n",
    "                        # Pair with user agent specified in csv file\n",
    "                        headers={\"User-Agent\": req[\"ua\"]},\n",
    "                        # Sets splash_url to whatever the current proxy that goes with current URL  is instead of actual splash url\n",
    "                        splash_url = req[\"ip\"],\n",
    "                        priority = x,\n",
    "                        meta={'priority': x}\n",
    "                        )\n",
    "\n",
    "    # Scraping function that will scrape URLs for specified information\n",
    "    def parse(self, response):\n",
    "\n",
    "        # Initialize item to function GameItem located in items.py, will be called multiple times\n",
    "        item = GameItem()\n",
    "        # Initialize saved_name\n",
    "        saved_name = \"\"\n",
    "        # Extract card category from URL using html code from website that identifies the category.  Will be outputted before rest of data\n",
    "        item[\"Category\"] = response.css(\"span.titletext::text\").get()\n",
    "        # For loop to loop through HTML code until all necessary data has been scraped\n",
    "        for game in response.css(\"tr[class^=deckdbbody]\"):\n",
    "            # Initialize saved_name to the extracted card name\n",
    "            saved_name  = game.css(\"a.card_popup::text\").get() or saved_name\n",
    "            # Now call item and set equal to saved_name and strip leading '\\n' from output\n",
    "            item[\"Card_Name\"] = saved_name.strip()\n",
    "            # Check to see if output is null, in the case that there are two different conditions for one card\n",
    "            if item[\"Card_Name\"] != None:\n",
    "                # If not null than store value in saved_name\n",
    "                saved_name = item[\"Card_Name\"].strip()\n",
    "            # If null then set null value to previous card name since if there is a null value you should have the same card name twice\n",
    "            else:\n",
    "                item[\"Card_Name\"] = saved_name\n",
    "            # Call item again in order to extract the condition, stock, and price using the corresponding html code from the website\n",
    "            item[\"Condition\"] = game.css(\"td[class^=deckdbbody].search_results_7 a::text\").get()\n",
    "            item[\"Stock\"] = game.css(\"td[class^=deckdbbody].search_results_8::text\").get()\n",
    "            item[\"Price\"] = game.css(\"td[class^=deckdbbody].search_results_9::text\").get()\n",
    "            if item[\"Price\"] == None:\n",
    "                item[\"Price\"] = game.css(\"td[class^=deckdbbody].search_results_9 span[style*='color:red']::text\").get()\n",
    "\n",
    "            # Return values\n",
    "            yield item\n",
    "            # Finds next page button\n",
    "        next_page = response.xpath('//a[contains(., \"- Next>>\")]/@href').get()\n",
    "        # If it exists and there is a next page enter if statement\n",
    "        if next_page is not None:\n",
    "            # Go to next page\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "        return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={'username': 'example@gmail.com', 'password': 'password'},\n",
    "                callback=self.after_login\n",
    "                )\n",
    "\n",
    "    def after_login(self, response):\n",
    "\n",
    "        import pdb; pdb.set_trace()\n",
    "        if \"Error while logging in\" in response.body:\n",
    "            self.logger.error(\"Login failed!\")\n",
    "        else:\n",
    "            import pdb; pdb.set_trace()\n",
    "            self.logger.error(\"Login succeeded!\")\n",
    "            #scraping code will go here once I can login in succcessfully"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
